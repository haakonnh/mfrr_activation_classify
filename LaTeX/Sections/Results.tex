\section{Model Results}
During the project's course, various models were developed and trained on
differing datasets and feature sets. Much experimentation went into determining
the best combination of factors to optimize practical performance and utility.
Model iterations are evaluated based on practical metrics as well as comparisons
to a naive baseline model. \textbf{Should naive model be mentioned earlier?}  This model
serves as a benchmark, providing a reference point against which more complex
models can be compared. 

Two dataset timeframes were primarily used for model training and evaluation:
a post-March 4th 2025 dataset and a combined 2024-2025 dataset. The two-year 
dataset provides more data for training, potentially improving model performance
and generalization. However, it also introduces some inconsistencies due to
changes in data resolution over time, and the March 4th 2025 mFRR market transition 
\autocite{ConfirmationMFRREAM2025}. The shorter dataset avoids these issues but
offers less data for training. Both datasets have their advantages and drawbacks,
and during the project, models were trained and evaluated on both to assess their
performance under different conditions. 


\subsection{Naive Model}\label{sec:naive-model-results}
The naive model simply predicts that the activation state at time $t+4$ is the
same as at time $t-4$, representing a strictly persistence-based approach. This
approach is inspired by findings in literature discussed in Section
\ref{sec:literature-review}, which highlight the auto-correlated nature of
activation patterns \autocite{backePredictionsPricesVolumes2023}. Much of the 
motivation for this project is to explore whether more sophisticated models
can outperform auto-correlation-based baselines by capturing additional relevant
information from various features.


Table \ref{tab:naive_baseline} shows the classification report for the naive
model on the post-March 4th 2025 dataset test split. The model performs well,
with a F1-macro score of 0.55. One can directly infer from the metrics that, in
the test split at least, 62\% of down-activations, 61\% of no-activations, and
43\% of up-activations are persistent from $t-4$ to $t+4$. This does not
guarantee that it is persistent between these two time steps, but it is a strong
auto-correlation indicator. Similar persistence values were observed in the
entire dataset with 66\%, 64\%, and 39\%, respectively. Down and no-activations
are more persistent than up-activations, which makes up-activations even more
challenging to predict. The "up" class is already the least frequent class, so
the model has less data to learn from, and the lower persistence further
complicates accurate predictions. 

Interestingly, precision and recall are equal for all
classes. This can also be seen in the confusion matrix in Figure
\ref{fig:naive-model-confusion-matrix}, as it is close to being symmetric and
respective rows and columns have similar sums. This phenomenon occurs because
the confusion matrix basically represents a transition matrix for the activation
states ($t-4$ to $t+4$). The symmetry reflects that, for instance, the number of
none $\rightarrow$ up transitions is similar to up $\rightarrow$ none
transitions over the dataset. 

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\hline
Class & Precision & Recall & F1-score & Support \\
\hline
down         & 0.62 & 0.62 & 0.62 & 1961 \\
none         & 0.61 & 0.61 & 0.61 & 2519 \\
up           & 0.43 & 0.43 & 0.43 &  792 \\
\hline
accuracy     &      &      & 0.59 & 5272 \\
macro avg    & 0.55 & 0.55 & 0.55 & 5272 \\
weighted avg & 0.59 & 0.59 & 0.59 & 5272 \\
\hline
\end{tabular}
\caption{Classification report for the naive last-observed-class baseline model.}
\label{tab:naive_baseline}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/naive_model_confusion.png}
    \caption{Confusion matrix (row-normalized) for naive model on post-March 4th 
    2025 dataset.}
    \label{fig:naive-model-confusion-matrix}
\end{figure}


\subsection{Machine Learning Model Results}
Models were trained using the open-source AutoML framework AutoGluon-Tabular
\autocite{ericksonAutoGluonTabularRobustAccurate2020}. Much of the appeal of
AutoGluon lies in its method of ensembling multiple models and stacking them in
multiple layers. Thus, individual models do not need to be considered in
isolation, as the ensemble model often outperforms any single model. As a
consequence, it is difficult to present results for individual model runs, as
each iteration may consist of different features and model parameters. However,
model performance remained relatively consistent across different runs, not
improving drastically with new feature additions or model tuning. As a matter of
fact, in the final model iterations, singular CatBoost models performed
consistently on par with complex ensembles. Thus, since singular models are
faster to train and evaluate, CatBoost models were used to explore various data
and model configurations more extensively. Performance and evaluation of these
models are presented in the following sections.

\subsubsection{Model Performance}
Table \ref{tab:catboost-val-test} and Figure
\ref{fig:catboost-confusion-val-test} show a classifcation report and confusion
matrices for a CatBoost model trained on the final featured post-March 4th 2025
dataset. Although scores improved slightly during model development, test set
F1-macro scores consistently remained around 0.5-0.6. Across all model
iterations, F1-macro scores never deviated significantly from the naive baseline
model. The model does manage to catch notably more down-activations than the
naive model, with a recall of 0.69 compared to 0.62. However, up-activation
F1-scores are lowered. This highlights the imbalance challenge, and the
up-class's lower predictability. No model iteration managed to significantly
outperform the naive baseline on up-activation predictions.


\begin{table}[ht]
\centering
\caption{CatBoost performance on validation and test sets (classes: down, none, up).}
\label{tab:catboost-val-test}
\begin{tabular}{lcccc}
\hline
\textbf{Split / Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\hline
\multicolumn{5}{l}{\textbf{Validation}} \\
down & 0.59 & 0.71 & 0.64 & 1873 \\
none & 0.72 & 0.63 & 0.67 & 2653 \\
up   & 0.48 & 0.45 & 0.46 & 746 \\
\cmidrule(lr){1-5}
accuracy    &       &       & 0.63 & 5272 \\
macro avg   & 0.59  & 0.59  & 0.59 & 5272 \\
weighted avg& 0.64  & 0.63  & 0.62 & 5272 \\
\hline
\multicolumn{5}{l}{\textbf{Test}} \\
down & 0.62 & 0.69 & 0.65 & 1961 \\
none & 0.62 & 0.62 & 0.62 & 2519 \\
up   & 0.48 & 0.35 & 0.40 & 792 \\
\cmidrule(lr){1-5}
accuracy    &       &       & 0.61 & 5272 \\
macro avg   & 0.57  & 0.55  & 0.56 & 5272 \\
weighted avg& 0.60  & 0.61  & 0.60 & 5272 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/cat_confusion_val.png}
        \caption{Validation set}
        \label{fig:catboost-confusion-val}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/cat_confusion_test.png}
        \caption{Test set}
        \label{fig:catboost-confusion-test}
    \end{subfigure}
    \caption{Row-normalized confusion matrices for the CatBoost model on the
    validation and test splits of the post-March 4th 2025 dataset.}
    \label{fig:catboost-confusion-val-test}
\end{figure}

Although macro metrics indicate that the model performs only slightly better
than the naive baseline, there is evidence suggesting that the model not only
relies on persistence patterns. Precision and recall vary across classes, unlike
the naive model. Higher recall and same precision for down-activations indicates
that the model captures some down-activations that occur non-persistently, while
maintaining precision. This is promising as it indicates that the dataset
contain useful information beyond persistence patterns. Figure
\ref{fig:catboost-prev-to-pred-confusion-matrix} shows a cross-tabulation of
predicted classes at $t+4$ against actual classes at $t-4$, with accuracies for
each entry in parantheses. This visualization essentially shows how many
predictions the model makes based on persistence versus non-persistence, and how
accurate these predictions are. Most predictions are indeed based on
persistence, as indicated by the high values along the diagonal. However, there
are cases where the model predicts a different class than the one observed at
$t-4$. It is clear to see that the model struggles most with predicting
up-activations, as many of its attempt to predict transitions from none/down to
up fail (0.23 and 0.19 accuracy, respectively). However, the model manages to,
for instance, catch transitions from down to none with 0.66 accuracy. This is
promising, but it is clear that more work is needed to improve the model's
ability to predict non-persistent activation events.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Images/cat_prev_to_pred.png}
    \caption{Confusion matrix (row-normalized) for CatBoost model on post-March 4th
    2025 dataset.}
    \label{fig:catboost-prev-to-pred-confusion-matrix}
\end{figure}


\subsection{The 2025 Dataset}

Depending on the 
specific metrics used for evaluation, different models perform best. It was
quite early on discovered that Random Forest and Extra Trees models performed
best overall, so the results focus on these two models. The models were trained
and evaluated based on F1-score, with raw precision and recall also considered
after training. The aforemontentioned transition metric is also used to evaluate
the models, as it provides insight into the model's ability to predict
activation transitions specifically.

\subsubsection{Extra Trees}
Figure \ref{fig:xt-2025-highf1} shows the Precision-Recall curve for an Extra Trees
model trained on the 2025 dataset. Three dots are scattered on the curve, representing
the precision-recall pairs for three interesting thresholds: the threshold given
by the predictor leaderboard, the threshold that maximizes F1-score, and the threshold
that gives a recall of 0.5. It is interesting that the leaderboard threshold gives a
lower F1-score than the maximum F1-score threshold after training, as one would
expect the leaderboard threshold to be optimal. Nevertheless, all three thresholds
provide solid performance, with the maximum F1-score threshold achieving the best balance
between precision and recall. The threshold that gives a recall of 0.5 sacrifices some 
precision to achieve higher recall, while simultaneously improving the transition metric. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/2025-highf1_xt.png}
    \caption{Precision-Recall Curve for Extra Trees model trained on 2025 dataset with highest F1-score.}
    \label{fig:xt-2025-highf1}
\end{figure}

\begin{table}[H]
    % So i want a table consisting of the 3 thresholds and their respective precision, recall, f1-score and transition metric
    % and also transition metric     
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Threshold & Precision & Recall & F1-score & Transition Metric \\
        \midrule
        Leaderboard & 0.65 & 0.31 & 0.42 & 9.49\% \\
        Max F1-score & 0.49 & 0.45 & 0.47 & 19.76\% \\
        Recall = 0.5 & 0.41 & 0.50 & 0.45 & 28.06\% \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics for Extra Trees model on 2025 dataset at different thresholds.}
    \label{tab:xt-2025-metrics}
\end{table}

Table \ref{tab:xt-2025-metrics} summarizes the performance metrics for the Extra Trees
model on the 2025 dataset at the three different thresholds. The transition metric is
highly correlated with recall, as expected, since higher recall means more activation
events are correctly identified, leading to better transition detection. The recall-focused
threshold achieves a transition success rate of 28.06\%, significantly higher than the 
threshold proposed by the leaderboard, and 10\% higher than the maximum F1-score threshold.
With a precision of 0.41 at this threshold, the model is relatively precise, not 
overwhelmingly predicting activations, which is crucial.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Images/2025-highf1_xt_fi.png}
    \caption{Feature importance for Extra Trees model trained on 2025 dataset with highest F1-score.}
    \label{fig:xt-2025-fI}
\end{figure}

Figure \ref{fig:xt-2025-fI} shows the feature importance for the Extra Trees model.
An important concept to note is that feature importance is independent of the
chosen threshold. The feature importance indicates which features the model
contribute the most to the underlying probability estimates. Threshold adjustments
merely shift the decision boundary without altering the relative importance of the features.
The top three features are in this particular model all lag features, specifically
persistency and singular activation lag features. This suggests that the model
relies heavily on historical activation patterns to make its predictions. Although one of
the goals was to reduce the reliance on lag features, it is still important to catch on
to activation trends. Other important features include day-ahead price, intraday wind
forecasts, time-related features such as peak hour and working day indicators, and
various interaction features. All these features likely contribute to capturing the complex
dynamics influencing activation events.

\subsection{The 2024-2025 Dataset}
Some extra preprocessing is necessary when including data from 2024, as data often come
in yearly batches. Separate CSV (Comma Separated Values) files for 2024 and 2025
are, therefore, merged into a single dataset before further handling. This dataset was
used for most of the training and evaluation process, as it provides more data for the models
to learn from, potentially leading to better generalization and performance. This comes at
the cost of slightly inconsistent data. mFRR activation data, for instance, transitioned
from hourly to 15-minute resolution in mid-2024 (\textbf{specify date perhaps}). 
This introduces inconsistency and noise into the dataset, which could affect model performance.
However, the benefits of having a larger dataset likely outweigh the this drawback.

\subsubsection{CatBoost Models}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/CatBoost_Tuned-first.png}
    \caption{Feature importance for CatBoost model trained on 2024-2025 dataset after
     hyperparameter tuning.}
    \label{fig:catboost-2024-2025-fI}
\end{figure}

\textbf{I probably want a kind of sweep over performances on different 
hyperparamter combinations.}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/catboost_first_metrics.png}
    \caption{Performance metrics for CatBoost model on 2024-2025 dataset at 
    different thresholds.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/up-down-price-minus-da-catboost.png}
    \caption{Up/down price minus day-ahead price distribution for 
    CatBoost model trained on 2024-2025 dataset. quick\_multiclass\_cat\_hpo}
    \label{fig:catboost-2024-2025-updownprice}
\end{figure}

Figure \ref{fig:catboost-2025-4th-confidence-metrics} shows the performance metrics 
for a CatBoost model trained on data from March 4th, 2025. The metrics are evaluated
at specific dataset subsets determined by how confident the model is in its predictions.
For instance, at a confidence threshold of 0.6, only predictions where the model's
predicted probability for the chosen class is at least 0.6 are considered. This approach
allows for an analysis of how the model's performance varies with its confidence level.
As the confidence threshold increases, accuracy steadily improves. This is expected, as 
higher confidence predictions should generally be more reliable. F1-macro score dips,
however, at higher thresholds, especially beyond 0.7. The most likely reason for this is
that the model rarely is very confident in predicting the less frequent classes (up- and 
down-activations). Most of these predictions are no-activation predictions, resulting
in good accruacy (0.778 in this case). Assume that all of these predictions are no-activation
predictions (\textbf{can probably check this quickly}). Then, 22.2\% of the predictions
are false negatives for the up- and down-activation classes, leading to low recall (0 
in this case) and thus low F1-score for these classes. The overall F1-macro score, being
the average of the F1-scores for all classes, consequently drops as well.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/confidence_thresholds_metrics_cat_short.png}
    \caption{Performance metrics for CatBoost model on 2025 March 4th dataset
    at different confidence thresholds.}
    \label{fig:catboost-2025-4th-confidence-metrics}
\end{figure}

\subsection{Correlation}
I do not know where this will end up in the structure, but talking about correlation
between features is key. Analysis has been done on correlation using different 
methods. Explaining corelation. 