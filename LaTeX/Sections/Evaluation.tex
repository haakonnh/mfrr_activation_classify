\subsection{Data Splitting}
Proper data splitting is crucial for reliably evaluating machine learning
models. In this study, the dataset is partitioned into training, validation, and
test sets in chronological order, with proportions of 60\%, 20\%, and 20\%,
respectively (see Figure~\ref{fig:data-splitting}). This temporal split is
essential to avoid data leakage and to ensure that model performance is assessed
on truly unseen future data. The training set is used to fit the models, the
validation set is used for tuning and model selection during development, and
the test set is held out until the end for an unbiased final evaluation of
predictive performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/Data Splitting.png}
    \caption{Temporal data splitting into training, validation, and test sets.}
    \label{fig:data-splitting}
\end{figure}

Some variation of cross-validation has the potential provide more generalizable
results, as singular validation splits may not fully capture the variability in
time series data. Porras (2025) applied a time series cross-validation, ensuring
that the models were always trained on past data and validated on future data
\autocite{porrasShortTermForecastingMFRR}. An analysis of this is presented 
in the results chapter.



\subsection{Evaluation Framework}
Classification problems are often evaluated using accuracy, precision,
recall, and F1-score. These metrics are defined as follows, based on definitions 
visualized in Figure \ref{fig:true-false-positives-negatives} 
\autocite{ClassificationAccuracyRecall}:
\begin{itemize}
    \item \textbf{Accuracy}: The ratio of correctly predicted observations to
    the total observations. It is calculated as:
    \[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]
    where TP is true positives, TN is true negatives, FP is false positives,
    and FN is false negatives.
    \item \textbf{Precision}: The ratio of correctly predicted positive
    observations to the total predicted positive observations. It is
    calculated as:
    \[ \text{Precision} = \frac{TP}{TP + FP} \]
    \item \textbf{Recall}: The ratio of correctly predicted positive
    observations to all observations in the actual class. It is calculated as:
    \[ \text{Recall} = \frac{TP}{TP + FN} \]
    \item \textbf{F1-score}: The weighted average of Precision and Recall. It is
    calculated as:  
    \[ \text{F1-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \]   

\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Images/TrueFalsePosinegs.drawio.png}
    \caption{Confusion matrix illustrating true/false positives/negatives.}
    \label{fig:true-false-positives-negatives}
\end{figure}

While accuracy is a commonly used metric, it can be misleading in cases of
imbalanced datasets. For instance, if only 5\% of the data points belong to
the positive class, a model that always predicts the negative class would
achieve 95\% accuracy, but would be useless for identifying positive cases. In
such scenarios, precision, recall, and F1-score provide a more nuanced
evaluation of model performance, especially in applications where the costs of
false positives and false negatives differ significantly. In the context of
mFRR activation prediction, false negatives (failing to predict an activation)
may lead to missed opportunities for market participation, while false positives
(predicting an activation when there isn't one) could result in unnecessary
costs or penalties. Therefore, a balanced consideration of these metrics is
essential for developing an effective classification model. 

\paragraph{Multiclass evaluation.} When the classification is no longer binary,
but multiclass (three or more classes) which is the focus 
of this project, evaluation metrics must be adapted. 
Common approaches include macro-averaging, micro-averaging, and weighted
averaging. The weighted-averaged F1-score is calculated by taking the mean of all 
per-class F1-scores, weighted by their \textit{support}, i.e., the number of true 
instances for each class. This approach is appropriate if the class distribution 
is imbalanced, and it is desired to give more importance to the performance on the more
frequent classes. This is not the case in this project, as minority classes are of
great importance. Therefore, the macro-averaged F1-score is used, which treats all classes
equally regardless of their frequency. Macro-averaged F1-scores are calculated by 
computing the F1-score for each class independently and then taking the average of 
these scores. Micro-averaging, which aggregates the contributions of all classes to 
compute the average metric, is less commonly used for imbalanced datasets. 
Micro-averaging is analogous to
accuracy in binary classification, which is not suitable for this problem due to
class imbalance \autocite{grandiniMetricsMultiClassClassification2020}.

\paragraph{Confusion matrix.} The confusion matrix is a cross tabulation of 
predicted versus actual class labels. It provides a breakdown of correct and incorrect
predictions for each class, aligning correct predictions along the diagonal. What 
stands out in a confusion matrix is not just the overall accuracy, but also
the specific types of errors the model makes. For instance, in a three-class
classification problem, the confusion matrix can reveal if the model tends to
confuse certain classes more than others. 
An example confusion matrix for a three-class classification problem is shown in
Figure \ref{fig:confusion-matrix}. In this example, the model performs well on the 
'down' class and 'none' class, capturing most of the true instances while making few
false predictions. The 'up' class has four correct predictions but also three 
misclassifications as 'none' and one as 'down', indicating that the model struggles
to differentiate the 'up' class from the others.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/ConfusionMatrix.png}
    \caption{Example of a confusion matrix for a three-class classification problem.}
    \label{fig:confusion-matrix}
\end{figure}



\paragraph{Precision-recall trade-off.}
This project primarily focuses on maximizing the F1-score, as it balances
precision and recall, providing a comprehensive measure of the model's
performance in predicting mFRR activations. Accuracy is essentially neglected
due to the imbalanced nature of the dataset. Between precision and recall,
recall is slightly prioritized, as missing an activation prediction is
considered more detrimental than a false alarm in this context. To achieve a
high recall score, the model must be capable of identifying as many actual
activations as possible, even if it means occasionally predicting an activation
when there isn't one. The model must be gutsy and attempt to identify patterns
that indicate an upcoming activation, not just follow persistence-based trends.
If a model has no such pattern recognition capabilities, it is of little
use.

There is, however, a limit to how much recall can be prioritized. If the
model predicts an activation for most time intervals, it will achieve a high recall
but at the cost of precision, rendering it ineffective. Therefore, the model must
strike a balance, ensuring that it is both sensitive to actual activations and
specific enough to avoid excessive false positives. This balance is crucial for
the model's success in practical applications, where both types of errors have
significant implications. The exact precision-recall trade-off can be adjusted based
on the specific use-case. Three potential cases are outlined below:
\begin{itemize}
    \item \textbf{Case 1 - Recall-focused}: A recall-focused approach can be
    useful for analysis purposes, where the goal is to identify periods of
    increased risk for activations. In this case, the model can be optimized to
    achieve a high recall score, even if it means sacrificing precision. Such a
    model would be valuable for understanding the conditions that lead to
    activations, but may not be suitable for direct market participation due to
    the high number of false positives. This approach might even be the best
    for market participation as the ability to discover more activations could
    outweigh the costs of false positives.
    \item \textbf{Case 2 - Balanced approach}: For general applications,
    maintaining a balance between precision and recall is often desirable.
    The model can be optimized to achieve a high F1-score, ensuring that
    both metrics are adequately addressed. This involves fine-tuning the
    model's parameters and threshold settings to find an optimal trade-off. Ideally,
    this approach would be used, achieving good performance in both precision and recall,
    making the model versatile for various applications, including market
    participation. Achieving such results is, however, quite challenging as either
    precision or recall often needs to be sacrificed to some extent to
    improve the other.
    \item \textbf{Case 3 - High precision focus}: In situations where false
    positives carry significant costs, the model can be adjusted to
    prioritize precision. This may involve raising the confidence
    threshold for predicting an activation, reducing false positives but
    potentially missing some true activations. For multi-market actors,
    such a model is useful, as they can afford to be selective about which market
    to participate in, only bidding into activation markets when the model is very
    certain of an upcoming activation. 
\end{itemize}
In this project, the focus is primarily on Case 1 and Case 2, with an
emphasis on achieving a high F1-score while slightly prioritizing recall.
This approach aims to ensure that the model has a chance to give users
novel insights into mFRR activations, potentially providing a competitive edge in
market participation. 

\paragraph{The transition metric.}
The transition metric evaluates the model's ability to predict
\textit{transitions} between classes. This is measured by looking at all
sequential time interval pairs $t-4$ and $t+4$ with different class labels. If
the model predicted the class label for time interval $t+4$ correctly, it is
counted as a successfully predicted transition. This is a crucial metric as it
indicates its non-persistence predictive capabilities. 

\paragraph{Probability correctness.} Although the models make hard
classifications, they do so based on predicted probabilities for each class. It
is, therefore, useful to evaluate how well these predicted probabilities align
with actual outcomes. For example, if the model predicts an activation with a
probability of 0.8 and an activation does not occur, this should be considered a
more severe error than if the model falsely predicted an activation when the
predicted probabilities were more evenly distributed. 

\subsubsection{Adjusting Classification Thresholds}\label{sec:classification-threshold-adjustment}
Decision thresholds are the thresholds at which a model assigns class labels
based on predicted probabilities. Although classification models output discrete
class labels, they do so based on underlying predicted probabilities for each
class. These thresholds can be adjusted to optimize certain performance metrics,
such as precision, recall, or F1-scores. This is particularly important in
imbalanced classification problems, where the default threshold (usually 0.5)
may underpredict the minority class \autocite{33TuningDecision}. Figure
\ref{fig:pr-curve} illustrates a typical precision-recall curve for a binary
classification problem (up-activation vs. not up-activation), showing how
precision and recall vary with different classification thresholds. The red dot
indicates the point (threshold of 0.24) where the F1-score is maximized,
representing an optimal balance between precision and recall. It is important
that threshold tuning is not performed on the test set, as this would lead to
overfitting and an overly optimistic estimate of model performance. Instead,
threshold tuning should be carried out using the validation set, as is done in
this project, ensuring that the test set remains a completely unseen dataset for
final model evaluation.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/pr-curve.png}
    \caption{Typical binary Precision-Recall Curve with F1-score maximization point 
    indicated.}
    \label{fig:pr-curve}
\end{figure}

When expanding from binary to three-class classification (up, down, none), there
are more degrees of freedom when it comes to adjusting decision thresholds.
However, the main challenge remains the same: the model tends to underpredict
the minority class, which in this case is the ``up'' activation class. Thus, the
''up'' class is post-tuned on the validation set. In a multi-class setting, it
was observed that the model hesitated even more in predicting the minority
class, often achieving recall scores below 0.1 for the ``up'' class. An 
automatic F1-macro score maximization procedure by method of up-weighting the
``up'' class probabilities was therefore implemented to mitigate this issue.
Thus, the ''up'' class is predicted more often to the extent that the overall
F1-macro score is maximized on the validation set.

The rationale behind this method is that the model picks up on up-activation
patterns to some extent, but as these patterns occur infrequently, the model is
not confident enough to predict the ``up'' class often enough. By up-weighting
the predicted probabilities for the ``up'' class, the model is encouraged to
predict ``up'' more frequently, thereby increasing recall for this class. This
has the to be detrimental to other class predictions, but if the trade-off is
optimized for F1-macro score, the overall performance across all classes can be
improved. There is no guarantee that this tuning will generalize onto the test
set, but it provides a systematic way to address the underprediction of the
minority class based on validation set performance. 



\subsection{Model Selection}\label{sec:model-selection} AutoGluon was used as
the main machine learning framework for model training and evaluation. When
AutoGluon is given a featured dataset, it automatically trains and tunes
multiple stacked and ensembled models using different algorithms and
hyperparameter settings. It then evaluates the performance of each model on a
validation set and selects the best performing model based on a specified
metric, such as F1-macro \autocite{ericksonAutoGluonTabularRobustAccurate2020}.
This automated approach simplified the model selection process for this project,
allowing for more efficient experimentation and iteration over different dataset
and feature configurations. As such, selection of specific models was not a
primary focus, but rather the overall framework and methodology.

