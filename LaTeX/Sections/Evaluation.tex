
\subsection{Evaluation Framework}
Classification problems are often evaluated using accuracy, precision,
recall, and F1-score. These metrics are defined as follows 
\autocite{ClassificationAccuracyRecall}:
\begin{itemize}
    \item \textbf{Accuracy}: The ratio of correctly predicted observations to
    the total observations. It is calculated as:
    \[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]
    where TP is true positives, TN is true negatives, FP is false positives,
    and FN is false negatives.
    \item \textbf{Precision}: The ratio of correctly predicted positive
    observations to the total predicted positive observations. It is
    calculated as:
    \[ \text{Precision} = \frac{TP}{TP + FP} \]
    \item \textbf{Recall}: The ratio of correctly predicted positive
    observations to all observations in the actual class. It is calculated as:
    \[ \text{Recall} = \frac{TP}{TP + FN} \]
    \item \textbf{F1-score}: The weighted average of Precision and Recall. It is
    calculated as:  
    \[ \text{F1-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \]   

\end{itemize}

While accuracy is a commonly used metric, it can be misleading in cases of
imbalanced datasets. For instance, if only 5\% of the data points belong to
the positive class, a model that always predicts the negative class would
achieve 95\% accuracy, but would be useless for identifying positive cases. In
such scenarios, precision, recall, and F1-score provide a more nuanced
evaluation of model performance, especially in applications where the costs of
false positives and false negatives differ significantly. In the context of
mFRR activation prediction, false negatives (failing to predict an activation)
may lead to missed opportunities for market participation, while false positives
(predicting an activation when there isn't one) could result in unnecessary
costs or penalties. Therefore, a balanced consideration of these metrics is
essential for developing an effective classification model. 

\paragraph{Multiclass evaluation.} When the classification is no longer binary,
but multiclass (three or more classes) which is the focus 
of this project, evaluation metrics must be adapted. 
Common approaches include macro-averaging, micro-averaging, and weighted
averaging. The weighted-averaged F1-score is calculated by taking the mean of all 
per-class F1-scores, weighted by their \textit{support}, i.e., the number of true 
instances for each class. This approach is appropriate if the class distribution 
is imbalanced, and it is desired to give more importance to the performance on the more
frequent classes. This is not the case in this project, as minority classes are of
great importance. Therefore, the macro-averaged F1-score is used, which treats all classes
equally regardless of their frequency. Macro-averaged F1-scores are calculated by 
computing the F1-score for each class independently and then taking the average of 
these scores. Micro-averaging, which aggregates the contributions of all classes to 
compute the average metric, is less commonly used for imbalanced datasets. 
Micro-averaging is analogous to
accuracy in binary classification, which is not suitable for this problem due to
class imbalance \autocite{grandiniMetricsMultiClassClassification2020}.

\paragraph{Confusion matrix.} The confusion matrix is a cross tabulation of 
predicted versus actual class labels. It provides a breakdown of correct and incorrect
predictions for each class, aligning correct predictions along the diagonal. What 
stands out in a confusion matrix is not just the overall accuracy, but also
the specific types of errors the model makes. For instance, in a three-class
classification problem, the confusion matrix can reveal if the model tends to
confuse certain classes more than others. 
An example confusion matrix for a three-class classification problem is shown in
figure \ref{fig:confusion-matrix}. In this example, the model performs well on the 
'down' class and 'none' class, capturing most of the true instances while making few
false predictions. The 'up' class has four correct predictions but also three 
misclassifications as 'none' and one as 'down', indicating that the model struggles
to differentiate the 'up' class from the others.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/ConfusionMatrix.png}
    \caption{Example of a confusion matrix for a three-class classification problem.}
    \label{fig:confusion-matrix}
\end{figure}



\paragraph{Precision-recall trade-off.}
This project primarily focuses on maximizing the F1-score, as it
balances precision and recall, providing a comprehensive measure of the
model's performance in predicting mFRR activations. Accuracy is essentially 
neglected due to the imbalanced nature of the dataset. Between precision
and recall, recall is slightly prioritized, as missing an activation
prediction is considered more detrimental than a false alarm in this
context. Precision can also be optimized more easily as the confidence
threshold can be adjusted post-training to favor precision over recall or
vice versa. To achieve a high recall score, the model must be capable of
identifying as many actual activations as possible, even if it means
occasionally predicting an activation when there isn't one. The model must
be gutsy and attempt to identify patterns that indicate an upcoming activation,
not just follow recent activation history. This approach aims to ensure that
the model has practical utility in real-world market participation scenarios.
If the model had no such pattern recognition capabilities, it would be of little
use beyond simple statistical analysis of recent activation trends, which could
be performed without machine learning.

There is, however, a limit to how much recall can be prioritized. If the
model predicts an activation for most time intervals, it will achieve a high recall
but at the cost of precision, rendering it ineffective. Therefore, the model must
strike a balance, ensuring that it is both sensitive to actual activations and
specific enough to avoid excessive false positives. This balance is crucial for
the model's success in practical applications, where both types of errors have
significant implications. The exact precision-recall trade-off can be adjusted based
on the specific use-case. Three potential cases are outlined below:
\begin{itemize}
    \item \textbf{Case 1 - Recall-focused}: A recall-focused approach can be
    useful for analysis purposes, where the goal is to identify periods of
    increased risk for activations. In this case, the model can be optimized to
    achieve a high recall score, even if it means sacrificing precision. Such a
    model would be valuable for understanding the conditions that lead to
    activations, but may not be suitable for direct market participation due to
    the high number of false positives. This approach might even be the best
    for market participation as the ability to discover more activations could
    outweigh the costs of false positives.
    \item \textbf{Case 2 - Balanced approach}: For general applications,
    maintaining a balance between precision and recall is often desirable.
    The model can be optimized to achieve a high F1-score, ensuring that
    both metrics are adequately addressed. This involves fine-tuning the
    model's parameters and threshold settings to find an optimal trade-off. Ideally,
    this approach would be used, achieving good performance in both precision and recall,
    making the model versatile for various applications, including market
    participation. Achieving such results is, however, quite challenging as either
    precision or recall often needs to be sacrificed to some extent to
    improve the other.
    \item \textbf{Case 3 - High precision focus}: In situations where false
    positives carry significant costs, the model can be adjusted to
    prioritize precision. This may involve raising the confidence
    threshold for predicting an activation, reducing false positives but
    potentially missing some true activations. For multi-market actors,
    such a model is useful, as they can afford to be selective about which market
    to participate in, only bidding into activation markets when the model is very
    certain of an upcoming activation. 
\end{itemize}
In this project, the focus is primarily on Case 1 and Case 2, with an
emphasis on achieving a high F1-score while slightly prioritizing recall.
This approach aims to ensure that the model has a chance to give users
novel insights into mFRR activations, potentially providing a competitive edge in
market participation. 

\paragraph{The transition metric}
is one more evaluation metric which is important to consider for this
specific problem: how often the model manages to predict the start of an activation
streak. This is measured by looking at all sequential time interval pairs where
the first interval did not have an activation, but the second one did. If the model predicted
an activation for the second interval, it is counted as a successful prediction
\textit{transition}, which is how this metric will be referred to. This metric is important 
because it concretizes the model's ability to catch the onset of activation periods, which is
the most valuable aspect for market participants. If the model can often enough predict these
transitions, it can provide significant strategic value, even if its overall precision and
recall are not perfect.

\paragraph{Probability correctness.} Although the models make hard classifications, they
do so based on predicted probabilities for each class. It is, therefore, useful to evaluate 
how well these predicted probabilities align with actual outcomes. For example, if the
model predicts an activation with a probability of 0.8 and an activation does not occur,
this should be considered a more severe error than if the model falsely predicted an 
activation when the predicted probabilities were more evenly distributed. 

\subsubsection{Data Splitting}
Proper data splitting is crucial for evaluating the performance of machine learning models.
In this study, the dataset is divided into training, validation, and test sets based on
temporal order to prevent data leakage and ensure that the model is evaluated on
unseen data. The training set is used to train the model, the validation set is used
for internal hyperparameter tuning and model evaluation during development, while 
the test set is reserved for the final evaluation of the model's performance.



\textbf{Explain more.}

\subsubsection{Model Selection}
Many different models were considered and tested during the project. The main models
that were evaluated include Random Forest, Extra Trees, LightGBM, XGBoost, CatBoost, and
neural network models implemented using AutoGluon. Each model has its strengths and 
weaknesses, and their performance can vary depending on the specific characteristics of the
dataset and the problem at hand. \textbf{Explain more.}

\paragraph{Random Forest and Extra Trees}
Random Forest and Extra Trees are ensemble learning methods that combine multiple
decision trees to improve predictive performance and reduce overfitting. 

\paragraph{CatBoost}
CatBoost is a gradient boosting algorithm that was found to perform well with particular
feature sets and hyperparameter configurations 
\autocite{dorogushCatBoostGradientBoosting2018}. Random Forest and Extra Trees were generally
favoured throughout the project, but CatBoost provided competitive results in some scenarios.
Since 

\paragraph{XGBoost and LightGBM}
\textbf{Need to check this out - haven't really given them a real chance with hyperparameter
tuning and such yet, as they are suspected to not be ideal for contiguous time-series data.}

\subsubsection{Adjusting Classification Thresholds}
\textbf{REDUNDANT AS WE ONLY CONSIDER 3-CLASS CLASSIFICATION.}
A critical aspect of optimizing classification models involves adjusting the decision
thresholds to balance precision and recall effectively. It is often useful to find
the threshold that maximizes the F1-score, which is the harmonic mean of precision and recall.
This approach ensures that both false positives and false negatives are minimized,
leading to a more balanced model performance. This was implemented by evaluating the model's
performance across a range of thresholds and selecting the one that yielded the highest F1-score.
A typical precision-recall curve is shwon in figure \ref{fig:pr-curve}.
During training and model evaluation, this threshold was heavily utilized as a general indicator
of the model's performance. This streamlined the evaluation process, making it easier to compare
different models and configurations based on a single metric. It is important to note that while
maximizing the F1-score provides a balanced approach, this is not necessarily the optimal strategy
threshold for all applications. Depending on the specific use case, one might prioritize
either precision or recall more heavily.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/pr-curve.png}
    \caption{Typical Precision-Recall Curve with F1-score maximization point indicated.}
    \label{fig:pr-curve}
\end{figure}

\subsubsection{Decision-bias tuning}

After extending the prediction task from binary (up vs.\ none) to ternary
(up, down, none), the model performance on the ``up'' class dropped
substantially. This was expected, as discussed in Section~\ref{sec:class_imbalance},
because ``up'' activations are rare and therefore provide the model with far fewer
examples to learn from. As a result, the model became overly conservative and
almost always favoured predicting the majority classes.

To address this issue, a simple post-processing method—here referred to as
\emph{decision-bias tuning}—was applied. The idea is similar in spirit to
adjusting the decision threshold in binary classification, but adapted to the
three-class setting. Instead of changing the model itself, we gently shift its
decision boundary so that it becomes more willing to predict the ``up'' class.

The procedure is straightforward. After the model outputs its class
probabilities, the predicted probability for the ``up'' class is multiplied by a
bias factor (greater than one). This does not change the underlying model or its
learned parameters; it only adjusts how the final class label is chosen. If the
model was previously hesitating to predict ``up'' even when it assigned a
moderate probability to it, this adjustment makes such predictions more likely.

The strength of the bias is selected using the validation dataset. Several
candidate bias levels are tested, and for each of them we compute the resulting
F1-score for the ``up'' class by keeping the other classes unchanged. The bias
factor that gives the best validation performance is then used when making
predictions on the test set. This tuning process helps correct the model’s
natural tendency to favour the majority classes and provides a controlled way of
improving recall and overall F1-score for the under-represented ``up'' class,
without degrading the performance on the remaining classes more than necessary.


\paragraph{Intuition and benefits.}
This method effectively lowers the decision threshold for predicting the ``up'' class,
making the model more inclined to predict ``up'' when there is uncertainty. This will
improve recall for the ``up'' class, at the cost of precision. By selecting $\alpha$ based on
F1-macro score, we ensure that the trade-off between precision and recall across
all classes is optimized for practical
performance. 

\subsubsection{AutoGluon}