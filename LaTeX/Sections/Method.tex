\section{Methodology}
This section delves into the methodologies and techniques employed in this study. 

\subsection{Problem Setup}
\textbf{Explain how the problem is constructed in code - this will probably
be quite in-depth and technical.}



\subsection{Metrics}
Classification problems are often evaluated using accuracy, precision,
recall, and F1-score. These metrics are defined as follows 
\autocite{ClassificationAccuracyRecall}:
\begin{itemize}
    \item \textbf{Accuracy}: The ratio of correctly predicted observations to
    the total observations. It is calculated as:
    \[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]
    where TP is true positives, TN is true negatives, FP is false positives,
    and FN is false negatives.
    \item \textbf{Precision}: The ratio of correctly predicted positive
    observations to the total predicted positive observations. It is
    calculated as:
    \[ \text{Precision} = \frac{TP}{TP + FP} \]
    \item \textbf{Recall}: The ratio of correctly predicted positive
    observations to all observations in the actual class. It is calculated as:
    \[ \text{Recall} = \frac{TP}{TP + FN} \]
    \item \textbf{F1-score}: The weighted average of Precision and Recall. It is
    calculated as:  
    \[ \text{F1-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \]   
\end{itemize}

While accuracy is a commonly used metric, it can be misleading in cases of
imbalanced datasets. For instance, if only 5\% of the data points belong to
the positive class, a model that always predicts the negative class would
achieve 95\% accuracy, but would be useless for identifying positive cases. In
such scenarios, precision, recall, and F1-score provide a more nuanced
evaluation of model performance, especially in applications where the costs of
false positives and false negatives differ significantly. In the context of
mFRR activation prediction, false negatives (failing to predict an activation)
may lead to missed opportunities for market participation, while false positives
(predicting an activation when there isn't one) could result in unnecessary
costs or penalties. Therefore, a balanced consideration of these metrics is
essential for developing an effective classification model. 



\paragraph{Precision-recall trade-off}.
This project primarily focuses on maximizing the F1-score, as it
balances precision and recall, providing a comprehensive measure of the
model's performance in predicting mFRR activations. Accuracy is essentially 
neglected due to the imbalanced nature of the dataset. Between precision
and recall, recall is slightly prioritized, as missing an activation
prediction is considered more detrimental than a false alarm in this
context. Precision can also be optimized more easily as the confidence
threshold can be adjusted post-training to favor precision over recall or
vice versa. To achieve a high recall score, the model must be capable of
identifying as many actual activations as possible, even if it means
occasionally predicting an activation when there isn't one. The model must
be gutsy and attempt to identify patterns that indicate an upcoming activation,
not just follow recent activation history. This approach aims to ensure that
the model has practical utility in real-world market participation scenarios.
If the model had no such pattern recognition capabilities, it would be of little
use beyond simple statistical analysis of recent activation trends, which could
be performed without machine learning.

There is, however, a limit to how much recall can be prioritized. If the
model predicts an activation for most time intervals, it will achieve a high recall
but at the cost of precision, rendering it ineffective. Therefore, the model must
strike a balance, ensuring that it is both sensitive to actual activations and
specific enough to avoid excessive false positives. This balance is crucial for
the model's success in practical applications, where both types of errors have
significant implications. The exact precision-recall trade-off can be adjusted based
on the specific use-case. Three potential cases are outlined below:
\begin{itemize}
    \item \textbf{Case 1 - Recall-focused}: A recall-focused approach can be
    useful for analysis purposes, where the goal is to identify periods of
    increased risk for activations. In this case, the model can be optimized to
    achieve a high recall score, even if it means sacrificing precision. Such a
    model would be valuable for understanding the conditions that lead to
    activations, but may not be suitable for direct market participation due to
    the high number of false positives. This approach might even be the best
    for market participation as the ability to discover more activations could
    outweigh the costs of false positives.
    \item \textbf{Case 2 - Balanced approach}: For general applications,
    maintaining a balance between precision and recall is often desirable.
    The model can be optimized to achieve a high F1-score, ensuring that
    both metrics are adequately addressed. This involves fine-tuning the
    model's parameters and threshold settings to find an optimal trade-off. Ideally,
    this approach would be used, achieving good performance in both precision and recall,
    making the model versatile for various applications, including market
    participation. Achieving such results is, however, quite challenging as either
    precision or recall often needs to be sacrificed to some extent to
    improve the other.
    \item \textbf{Case 3 - High precision focus}: In situations where false
    positives carry significant costs, the model can be adjusted to
    prioritize precision. This may involve raising the confidence
    threshold for predicting an activation, reducing false positives but
    potentially missing some true activations. For multi-market actors,
    such a model is useful, as they can afford to be selective about which market
    to participate in, only bidding into activation markets when the model is very
    certain of an upcoming activation. 
\end{itemize}
In this project, the focus is primarily on Case 1 and Case 2, with an
emphasis on achieving a high F1-score while slightly prioritizing recall.
This approach aims to ensure that the model has a chance to give users
novel insights into mFRR activations, potentially providing a competitive edge in
market participation. 

\paragraph{The transition metric}
is one more evaluation metric which is important to consider for this
specific problem: how often the model manages to predict the start of an activation
streak. This is measured by looking at all sequential time interval pairs where
the first interval did not have an activation, but the second one did. If the model predicted
an activation for the second interval, it is counted as a successful prediction
\textit{transition}, which is how this metric will be referred to. This metric is important 
because it concretizes the model's ability to catch the onset of activation periods, which is
the most valuable aspect for market participants. If the model can often enough predict these
transitions, it can provide significant strategic value, even if its overall precision and
recall are not perfect.

\paragraph{Probability correctness.} Although the models make hard classifications, they
do so based on predicted probabilities for each class. It is, therefore, useful to evaluate 
how well these predicted probabilities align with actual outcomes. For example, if the
model predicts an activation with a probability of 0.8 and an activation does not occur,
this should be considered a more severe error than if the model falsely predicted an 
activation when the predicted probabilities were more evenly distributed. 

\subsection{Data Splitting}
Proper data splitting is crucial for evaluating the performance of machine learning models.
In this study, the dataset is divided into training, validation, and test sets based on
temporal order to prevent data leakage and ensure that the model is evaluated on
unseen data. The training set is used to train the model, the validation set is used
for hyperparameter tuning and model selection, and the test set is reserved for the final
evaluation of the model's performance. \textbf{Explain more.}

\subsection{Model Selection}
Many different models were considered and tested during the project. The main models
that were evaluated include Random Forest, Extra Trees, LightGBM, XGBoost, CatBoost, and
neural network models implemented using AutoGluon. Each model has its strengths and 
weaknesses, and their performance can vary depending on the specific characteristics of the
dataset and the problem at hand. \textbf{Explain more.}

\subsubsection{Random Forest and Extra Trees}
Random Forest and Extra Trees are ensemble learning methods that combine multiple
decision trees to improve predictive performance and reduce overfitting. 

\subsubsection{CatBoost}
CatBoost is a gradient boosting algorithm that was found to perform well with particular
feature sets and hyperparameter configurations 
\autocite{dorogushCatBoostGradientBoosting2018}. Random Forest and Extra Trees were generally
favoured throughout the project, but CatBoost provided competitive results in some scenarios.
Since 

\subsubsection{XGBoost and LightGBM}
\textbf{Need to check this out - haven't really given them a real chance with hyperparameter
tuning and such yet, as they are suspected to not be ideal for contiguous time-series data.}

\subsection{Adjusting Classification Thresholds}
A critical aspect of optimizing classification models involves adjusting the decision
thresholds to balance precision and recall effectively. It is often useful to find
the threshold that maximizes the F1-score, which is the harmonic mean of precision and recall.
This approach ensures that both false positives and false negatives are minimized,
leading to a more balanced model performance. This was implemented by evaluating the model's
performance across a range of thresholds and selecting the one that yielded the highest F1-score.
A typical precision-recall curve is shwon in figure \ref{fig:pr-curve}.
During training and model evaluation, this threshold was heavily utilized as a general indicator
of the model's performance. This streamlined the evaluation process, making it easier to compare
different models and configurations based on a single metric. It is important to note that while
maximizing the F1-score provides a balanced approach, this is not necessarily the optimal strategy
threshold for all applications. Depending on the specific use case, one might prioritize
either precision or recall more heavily.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Images/pr-curve.png}
    \caption{Typical Precision-Recall Curve with F1-score maximization point indicated.}
    \label{fig:pr-curve}
\end{figure}

\subsection{Decision-bias tuning}
Simply adjusting the classification threshold to maximize F1-score is simple in a binary
classification setting, but in a ternary classification setting, where there are three classes,
such a straightforward approach is not possible. The 'up' class is, as discussed in 
section \ref{sec:class_imbalance}, a heavily imbalanced class. Imbalanced classes are often
more difficult for models to learn, as crucial information about the minority class is more
sparse. After the switch from binary to ternary classification, it was observed that the model's
performance on the 'up' class degraded significantly. To mitigate this, a decision-bias tuning
approach was implemented. 

Decision-bias tuning is very similar to adjusting classification thresholds, as both methodologies
tune the model's decision-making process post-training. Decision-bias tuning specifically focuses on
modifying the decision boundary for the 'up' class to improve its F1 score. 
% --- Multiclass "up" decision-bias and validation-based tuning ---

\paragraph{Class-specific decision bias for ``up''.}
Given predicted class probabilities $p_c(x)$ for \\ $c \in \{\text{up}, \text{down}, \text{none}\}$,
we adjust the score for the ``up'' class by a multiplicative factor $\alpha > 1$:
\[
  p'_{\text{up}}(x) \;=\; \alpha \, p_{\text{up}}(x), \qquad
  p'_k(x) \;=\; p_k(x)\ \ \text{for}\ k \neq \text{up},
\]
and predict
\[
  \hat{y}(x) \;=\; \arg\max_{c} \; p'_c(x).
\]
For $\alpha > 1$, the effective decision threshold for predicting ``up'' is lowered:
\[
  p_{\text{up}}(x) \;\ge\; \max_{k \neq \text{up}} \frac{p_k(x)}{\alpha}.
\]

\paragraph{Validation-based selection of $\alpha$.}
Let $\mathcal{A}$ be a finite candidate set (e.g., $\mathcal{A} = \{1.0,\,1.25,\,1.5,\,2.0,\,3.0\}$).
We select $\alpha$ by grid search on a validation set $\mathcal{D}_{\mathrm{val}}$ to maximize the
$\mathrm{F1}$ score for the ``up'' class:
\begin{enumerate}
  \item For each $\alpha \in \mathcal{A}$:
  \begin{enumerate}
    \item Compute adjusted scores $p'(x;\alpha)$ by multiplying $p_{\text{up}}(x)$ by $\alpha$ and leaving other classes unchanged.
    \item Predict $\hat{y}(x;\alpha) = \arg\max_{c} p'_c(x;\alpha)$ for $x \in \mathcal{D}_{\mathrm{val}}$.
    \item Compute $\mathrm{F1}_{\text{up}}(\alpha)$ by treating ``up'' as positive and $\{\text{down}, \text{none}\}$ as negative.
  \end{enumerate}
  \item Choose
  \[
    \alpha^{*} \;\in\; \arg\max_{\alpha \in \mathcal{A}} \;\mathrm{F1}_{\text{up}}(\alpha).
  \]
  \item Use $\alpha^{*}$ at inference time to adjust $p_{\text{up}}(x)$ before the final $\arg\max$.
\end{enumerate}

% --- Optional: Algorithm float (requires \usepackage[ruled,vlined]{algorithm2e}) ---
% \begin{algorithm}[H]
% \caption{Validation-based tuning of the ``up'' bias multiplier $\alpha$}
% \KwIn{Validation data $\mathcal{D}_{\mathrm{val}}$, candidate set $\mathcal{A}$, class posteriors $p_c(x)$}
% \KwOut{$\alpha^\*$}
% $\text{bestF1} \leftarrow -\infty$, $\alpha^\* \leftarrow 1.0$\;
% \ForEach{$\alpha \in \mathcal{A}$}{
%   \ForEach{$x \in \mathcal{D}_{\mathrm{val}}$}{
%     $p'_{\text{up}}(x) \leftarrow \alpha \cdot p_{\text{up}}(x)$; \quad $p'_k(x) \leftarrow p_k(x)$ for $k \neq \text{up}$\;
%     $\hat{y}(x;\alpha) \leftarrow \arg\max_c p'_c(x)$\;
%   }
%   $\text{F1}_{\text{up}}(\alpha) \leftarrow \text{F1}(\{\hat{y}(x;\alpha)\}, \{y(x)\}; \text{up vs not-up})$\;
%   \If{$\text{F1}_{\text{up}}(\alpha) > \text{bestF1}$}{
%     $\text{bestF1} \leftarrow \text{F1}_{\text{up}}(\alpha)$; \quad $\alpha^\* \leftarrow \alpha$\;
%   }
% }
% \Return{$\alpha^\*$}\;
% \end{algorithm}

\paragraph{Intuition and benefits.}
This method effectively lowers the decision threshold for predicting the ``up'' class,
making the model more inclined to predict ``up'' when there is uncertainty. This will
improve recall for the ``up'' class, at the cost of precision. By selecting $\alpha$ based on
F1 score, we ensure that the trade-off between precision and recall is optimized for practical
performance. 



\subsection{AutoGluon}
